<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Projects | A Fuzzy Philosopher</title>
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/baguettebox.js/1.11.0/baguetteBox.min.css" integrity="sha256-cKiyvRKpm8RaTdU71Oq2RUVgvfWrdIXjvVdQF2oZ1Y4=" crossorigin="anonymous">
<link href="../../assets/css/all.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:700,900" rel="stylesheet">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://farley.futuresight.io/pages/projects/">
<link rel="icon" href="../../favicon.ico" sizes="16x16">
<!--[if lt IE 9]><script src="https://html5shim.googlecode.com/svn/trunk/html5.js"></script><![endif]--><meta name="author" content="Farley Lai">
<meta property="og:site_name" content="A Fuzzy Philosopher">
<meta property="og:title" content="Projects">
<meta property="og:url" content="http://farley.futuresight.io/pages/projects/">
<meta property="og:description" content="Dict Eye







This app intended to integrate with Google glasses complements our eXtream Language Learning system (xLL). Imagine the
camera serves as our eyes to capture vocabulary from all kinds of">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-01-19T16:38:29-05:00">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Header and menu bar -->
<div class="container">
      <header class="blog-header py-3"><div class="row nbb-header align-items-center">
          <div class="col-md-3 col-xs-2 col-sm-2" style="width: auto;">
            <button class="navbar-toggler navbar-light bg-light nbb-navbar-toggler" type="button" data-toggle="collapse" data-target=".bs-nav-collapsible" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse bs-nav-collapsible bootblog4-search-form-holder">
                
            </div>
        </div>
          <div class="col-md-6 col-xs-10 col-sm-10 bootblog4-brand" style="width: auto;">
            <a class="navbar-brand blog-header-logo text-dark" href="http://farley.futuresight.io/">

            <span id="blog-title">A Fuzzy Philosopher</span>
        </a>
          </div>
            <div class="col-md-3 justify-content-end align-items-center bs-nav-collapsible collapse flex-collapse bootblog4-right-nav">
            <nav class="navbar navbar-light bg-white"><ul class="navbar-nav bootblog4-right-nav"></ul></nav>
</div>
    </div>
</header><nav class="navbar navbar-expand-md navbar-light bg-white static-top"><div class="collapse navbar-collapse bs-nav-collapsible" id="bs-navbar">
            <ul class="navbar-nav nav-fill d-flex w-100">
<li class="nav-item">
<a href="../projects.html" class="nav-link">Projects</a>
                </li>
<li class="nav-item">
<a href="../research.html" class="nav-link">Research</a>

                
            </li>
</ul>
</div>
<!-- /.navbar-collapse -->
</nav>
</div>

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Projects</a></h1>

        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<div class="sect1">
<h3 id="_dict_eye">Dict Eye</h3>
<div class="sectionbody">
<div class="videoblock">
<div class="content">
<iframe width="480" height="360" src="https://www.youtube.com/embed/v2haz09SM38?rel=0" frameborder="0" allowfullscreen></iframe>
</div>
</div>
<div class="paragraph">
<p>This app intended to integrate with Google glasses complements our eXtream Language Learning system (xLL). Imagine the
camera serves as our eyes to capture vocabulary from all kinds of visual media. We demonstrate words and phrases from
subtitles can be recognized according to a specified learner’s level and show definitions in real-time. Vocabulary
recognition statistics are collected for users to review in flashcards with spaced repetition after finishing watching
the media. The real-time text tracking is supported by Qualcomm Vuforia.</p>
</div>
</div>
</div>
<div class="sect1">
<h3 id="_exploration_of_text_independent_speaker_identification_methods">Exploration of Text-Independent Speaker Identification Methods</h3>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>(55:145) Pattern Recognition, Fall13, score A+</p>
</li>
<li>
<p><a href="https://drive.google.com/a/dynagrid.net/file/d/0B4XuqKXDPDpRVEVibm9HSFplZzA/edit">Report</a></p>
</li>
<li>
<p><a href="https://goo.gl/VnZrKc">Slides</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This work is motivated by the EgoSense project which aims at constructing social networks in real life by capturing
human activities and interactions to monitor long-term user behavior patterns. Unlike online social networking services
such as facebook, twitter and Google+, where convenient and efficient authentication of users is available,
identification of human targets in real life remains challenging because of requirements of accuracy, robustness and
least obtrusiveness. In view of least obtrusiveness, biometric recognition techniques based on face, fingerprint and
voice are widely studied but the accuracy and robustness are usually limited for practical use due to insufficient
training data for generalization and data collection channel dependency. In this project, we particularly focus on
text-independent speaker identification of human voice. Its application can be seen in a customer relationship
management system (CRM) such as a call center where users typically do not stay for long to contribute speech more than
30 seconds for training and the system is expected to recognize a user quickly in 2 seconds. Though the technque has
been comprehensively studied, most variants are based on the state of the art guassian mixture model (GMM) to represent
speakers. Our goal is to explore and compare the performance of three main speaker identification methods. The first
method serves as the baseline GMM without special adaptation and incorporating other techniques. The second method
employs the universal background model (UBM) to highlight the differentiation between a speaker under test and the
others. The third method utilizes the discriminating power of support vector machines (SVMs) to facilitate
classifying confusing features. Besides the methods, we also present relevant implementation details of feature
extraction and speech detection which are essential preprocessing to ensure good speaker identification performance.</p>
</div>
</div>
</div>
<div class="sect1">
<h3 id="_comparison_of_image_registration_methods">Comparison of Image Registration Methods</h3>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>(55:248) Advanced Digital Image Processing, Spr.13, score A</p>
</li>
<li>
<p><a href="https://docs.google.com/a/dynagrid.net/file/d/0B4XuqKXDPDpROU9hb28yU1NiMDQ/view">Report</a></p>
</li>
<li>
<p><a href="https://docs.google.com/a/dynagrid.net/file/d/0B4XuqKXDPDpRRlQ5VGZnVEJHU3M/view">Slides</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Image registration is the process to align corresponding points in two different image spaces.
These images can be acquired from different sensors, views, time or dimensions.
In this project, we evaluated four image registration methods which are performed on cross-modality datasets, including
point-based rigid transformation (PB1), point-based transformation with anisotropic scaling (PB2),
iterative closest point algorithm (ICP) and intensity-based registration using mutual information (MI).
PB1 and PB2 require manually selected corresponding fiducial points as the input and expected output in both image spaces.
The methods compute the transformation iteratively by minimizing the root mean squre errors (RMSE) between the points in one image space
and those corresponding ones in the other space. For the ICP, two corresponding and partially overlapped surfaces in both
image spaces are selected and converted to point sets as the input and expected output.
The algorithm computes the transformation by optimizing the weighted average distance between the two surfaces in each iteration. Though all the four
methods compute the transformantions based on different measures,
we intend to compare them by deriving common perforamance indices such as
sum of squared differences (SSD), correslation coefficient (CC), joint entropy (H),
normalized mutual information (NMI) and employ t-test to show pairwise statistical significance.</p>
</div>
</div>
</div>
<div class="sect1">
<h3 id="_light_field_contour_tracking">Light Field Contour Tracking</h3>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>(22c:196) Wireless Sensor Networks, Fall12, score A-</p>
</li>
<li>
<p><a href="https://github.com/farleylai/wsn-contour-tracking">GitHub</a></p>
</li>
<li>
<p><a href="https://drive.google.com/a/dynagrid.net/file/d/0B4XuqKXDPDpRNzJmRFRPTnIxY1k/view?pli=1">Report</a></p>
</li>
<li>
<p><a href="https://drive.google.com/a/dynagrid.net/file/d/0B4XuqKXDPDpRM3ZOMTN0cS1vNkE/view?pli=1">Slides</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This project will be to study and implement different algorithms for tracking contours on wireless sensor networks. To
achieve the goal, we will need to set up a grid where a mote with a light sensor is located at each vertex. Each mote
will be sending feedback to our laptop where all the data will be analyzed. We will estimate how the light is being
dispersed throughout the grid using the intensity of the light sensed at the vertexes.</p>
</div>
</div>
</div>
<div class="sect1">
<h3 id="_construct_a_small_world_bittorrent_swarm">Construct a Small-World BitTorrent Swarm</h3>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>(22c:196) Computational Epidemiology, Spr.12</p>
</li>
<li>
<p><a href="https://drive.google.com/a/dynagrid.net/file/d/0B4XuqKXDPDpRZGxKa0tKbnBhUnM/view?pli=1">Report</a></p>
</li>
<li>
<p><a href="https://docs.google.com/presentation/d/1h-SZ3U-94LzqoloC7WRBXPYRoDbV74954na2oXVxS2o/edit?usp=sharing">Slides</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Recent studies have shown a BitTorrent swarm may not be a small-world network
because the peer selection strategy of the tracker is to assign a set of known peers randomly
regardless of individual characteristics. We propose a tracker side peer selection approach based on the file download
completion rank to generate a set of peers for a newly attending peer.
The resulting swarm is expected to have small-world properties such as short average path lengths and larger clustering coefficients.
Our simulation results demonstrate that a swarm with small-world properties generally performs better
than random graph based swarms in terms of the piece hop availability measure despite large numbers of peers leaving.</p>
</div>
</div>
</div>
</div>
    </div>
    

</article><!--End of body content--><footer id="footer">
            Contents © 2019 <a href="mailto:farleylai%5Bat%5Dfuturesight.io">Farley Lai</a> 
<a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">
<img alt="Creative Commons License BY-NC-ND" style="border-width:0; margin-bottom:12px;" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png"></a>
            
        </footer>
</div>
</div>


        <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha256-ZvOgfh+ptkpoa2Y4HkRY28ir89u/+VRyDE7sB7hEEcI=" crossorigin="anonymous"></script><script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/baguettebox.js/1.11.0/baguetteBox.min.js" integrity="sha256-yQGjQhFs3LtyiN5hhr3k9s9TWZOh/RzCkD3gwwCKlkg=" crossorigin="anonymous"></script><script src="../../assets/js/all.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
